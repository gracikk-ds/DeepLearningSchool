{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "# Import Image manipulation\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Import data visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аугментация позволяет расширить возможное количество обучающих примеров, снизить переобучение, увеличить выборку. Таким образом, мы должны обучать нейронную сеть на аугментированных картинках из **обучающей выборки (train)**. Проверять ее результат на неаугментированных картинках из **валидационной выборки (val)** и давать предсказание на неаугментированных из **тестовой (test)**, которая идет в score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpsonsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Датасет с картинками, который паралельно подгружает их из папок\n",
    "    производит скалирование и превращение в торчевые тензоры\n",
    "    \"\"\"\n",
    "    # Получаем на вход список путей к файлам, которые нужно подгрузить *files*\n",
    "    # А также указываем какой тип датасета мы используем *DATA_MODES*\n",
    "    def __init__(self, files, mode):\n",
    "        super().__init__()\n",
    "        # список файлов для загрузки\n",
    "        self.files = sorted(files)\n",
    "        # режим работы\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode not in DATA_MODES:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
    "            raise NameError\n",
    "        \n",
    "        # Определяем переменную для метода длины датасета\n",
    "        self.len_ = len(self.files)\n",
    "        \n",
    "        # Определяем LabelEncoder, который по имени картинки преобразует ее в число\n",
    "        # self.label_encoder = LabelEncoder()\n",
    "        \n",
    "        # Лейблы тестового набора фалов не известны \n",
    "        if self.mode != 'test':\n",
    "            self.labels = [path.parent.name for path in self.files]\n",
    "            self.label_encoder.fit(self.labels)\n",
    "            \n",
    "            # тут мы пользуемся странной библиотекой\n",
    "            with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
    "                  pickle.dump(self.label_encoder, le_dump_file)\n",
    "                      \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        # Тут мы открываем нашу картинку\n",
    "        image = Image.open(file)\n",
    "        # Выгружаем пиксели\n",
    "        image.load()\n",
    "        return image\n",
    "    \n",
    "    def prepare_sample(self, image):\n",
    "\n",
    "        H = image.size[1]\n",
    "        W = image.size[0]\n",
    "        \n",
    "        if H > W:\n",
    "            image = transforms.functional.pad(image,padding=[round(abs(H-W)/2),0])\n",
    "        else:\n",
    "            image = transforms.functional.pad(image,padding=[0,round(abs(H-W)/2)])\n",
    "            \n",
    "        return np.array(image)\n",
    "    \n",
    "    def augmentation_pipline(self, image):\n",
    "        \"\"\"Аугментирующие трансформации для трейна\"\"\"\n",
    "        \n",
    "        image = np.asarray(image)\n",
    "        \n",
    "        augmentation_pipeline = A.Compose(\n",
    "            [\n",
    "                A.HorizontalFlip(p = 0.5), # apply horizontal flip to 50% of images\n",
    "                A.OneOf(\n",
    "                    [\n",
    "                        # apply one of transforms to 50% of images\n",
    "                        A.RandomContrast(limit=0.2), # apply random contrast\n",
    "                        A.RandomGamma(), # apply random gamma\n",
    "                        A.RandomBrightness(limit=0.2), # apply random brightness\n",
    "                    ],\n",
    "                    p = 0.5\n",
    "                ),\n",
    "                A.OneOf(\n",
    "                    [\n",
    "                        # apply one of transforms to 50% images\n",
    "                        A.ElasticTransform(\n",
    "                            alpha = 120,\n",
    "                            sigma = 120 * 0.09,\n",
    "                            alpha_affine = 120 * 0.03\n",
    "                        ),\n",
    "                        # A.GridDistortion(),\n",
    "                        A.OpticalDistortion(\n",
    "                            distort_limit = 0.7,\n",
    "                            shift_limit = 0.5\n",
    "                        ),\n",
    "                    ],\n",
    "                    p = 0.5\n",
    "                ),\n",
    "                \n",
    "            ],\n",
    "            p = 1\n",
    "        )\n",
    "        \n",
    "        image = augmentation_pipeline(image = image)\n",
    "        image = Image.fromarray(image[\"image\"])\n",
    "        return image\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        # Перевод в нужный размер и в тензоры\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.Resize(size=(RESCALE_SIZE+16, RESCALE_SIZE+16)),\n",
    "            transforms.RandomRotation(degrees=5),\n",
    "            transforms.RandomCrop(size=(RESCALE_SIZE, RESCALE_SIZE)),\n",
    "            transforms.ToTensor(), # переводит в интервал от 0 до 1\n",
    "            # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Resize(size=(RESCALE_SIZE, RESCALE_SIZE)),\n",
    "            transforms.ToTensor(), # переводит в интервал от 0 до 1\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "        # Загружаем картинку по пути, используя конкретный индекс\n",
    "        x = self.load_sample(self.files[index])\n",
    "        \n",
    "        # Если тест, то лейблы не нужны\n",
    "        if self.mode == 'test':\n",
    "            x = transform_test(x)\n",
    "            return x\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            label_id = self.label_encoder.transform([label])\n",
    "            y = label_id.item()\n",
    "            x = prepare_sample(x)\n",
    "            x = self.augmentation_pipline(x)\n",
    "            x = transform_train(x)\n",
    "            return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить веса нашей нейросети model\n",
    "torch.save(model.state_dict(), \"path_to\\\\model_weights.pth\") \n",
    "\n",
    "# or\n",
    "\n",
    "PATH = \"path_to\\\\model_weights.pth\"\n",
    "torch.save({\n",
    "            'modelA_state_dict': netA.state_dict(),\n",
    "            'modelB_state_dict': netB.state_dict(),\n",
    "            'optimizerA_state_dict': optimizerA.state_dict(),\n",
    "            'optimizerB_state_dict': optimizerB.state_dict(),\n",
    "            }, PATH)\n",
    "\n",
    "# загружаем сохраненное состояние весов нейросети\n",
    "model.load_state_dict(torch.load(\"path_to\\\\model_wights.pth\"))\n",
    "model.train() # переключаем нейросеть в режим обучения\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "modelA.load_state_dict(checkpoint['modelA_state_dict'])\n",
    "modelB.load_state_dict(checkpoint['modelB_state_dict'])\n",
    "optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
    "optimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def find_lr(model, dataloaders, loss_fn, optimizer, init_value=1e-8, final_value=10.0, use_gpu=True):\n",
    "    \"\"\"Слегка модифицированная функция для поиска оптимального learning rate \n",
    "    функция взята из замечатлеьной книги книги \"Ian Pointer - Programming PyTorch\n",
    "    for Deep Learning - Creating and Deploying Deep Learning Applications-\n",
    "    O’Reilly Media (2019)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    number_in_epoch = len(dataloaders['train']) - 1\n",
    "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    best_loss = 0.0\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for inputs, labels in dataloaders['train']:\n",
    "        if use_gpu:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "        batch_num += 1\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Crash out if loss explodes\n",
    "\n",
    "        if batch_num > 1 and loss > 4 * best_loss:\n",
    "            return log_lrs[10:-5], losses[10:-5]\n",
    "\n",
    "        # Record the best loss\n",
    "\n",
    "        if loss < best_loss or batch_num == 1:\n",
    "            best_loss = loss\n",
    "\n",
    "        # Store the values\n",
    "\n",
    "        losses.append(loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "\n",
    "        # Do the backward pass and optimize\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the lr for the next step and store\n",
    "\n",
    "        lr *= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "    return log_lrs[10:-5], losses[10:-5]\n",
    "\n",
    "# подбор оптимального lr для классификатора model_vgg16_bn.classifier\n",
    "logs, losses = find_lr(model_vgg16_bn, dataloaders, loss_fn, optimizer, init_value=1e-8, final_value=10.0)\n",
    "\n",
    "# построим график для оптимального подбора lr\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(logs,losses)\n",
    "ax.set_xlabel(\"$10^x$\")\n",
    "ax.set_ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "    \n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param512 = model_vgg16_bn.features[27:44].parameters()\n",
    "param256 = model_vgg16_bn.features[17:27].parameters()\n",
    "param128 = model_vgg16_bn.features[10:17].parameters()\n",
    "param64 = model_vgg16_bn.features[0:10].parameters()\n",
    "\n",
    "# В качестве cost function используем кросс-энтропию\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "found_lr = lr=1e-3\n",
    "\n",
    "# Дифференциальное обучение (по группам слоев)у каждой группы свой lr\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "{ 'params': model_vgg16_bn.classifier.parameters(), 'lr': found_lr},\n",
    "{ 'params': param512, 'lr': found_lr / 3},\n",
    "{ 'params': param256, 'lr': found_lr / 10},\n",
    "{ 'params': param128, 'lr': found_lr / 50},\n",
    "{ 'params': param64, 'lr': found_lr / 100},\n",
    "], lr=found_lr / 100, amsgrad=True)\n",
    "\n",
    "# Использовать ли GPU\n",
    "model_vgg16_bn = model_vgg16_bn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accurancy_for_each_class(y_test_all, predictions_all):\n",
    "    class_correct = [0 for i in range(classes_number)]\n",
    "    class_total = [0 for i in range(classes_number)]\n",
    "    feature_names = sorted(set(dataloaders['val'].dataset.labels))\n",
    "\n",
    "    c = (predictions_all == y_test_all).squeeze()\n",
    "    for i in range(len(predictions_all)):\n",
    "        label = predictions_all[i]            \n",
    "        class_correct[label] += c[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "    print(class_total)\n",
    "    print(len(class_total))\n",
    "\n",
    "    for i in range(classes_number):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            (feature_names[i], (100 * class_correct[i] / class_total[i]) if class_total[i] != 0 else -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnsemble(nn.Module):   \n",
    "    def __init__(self, modelA, modelB):\n",
    "        super(MyEnsemble, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.classifier = nn.Linear(classes_number * 2, classes_number)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.modelA(x)\n",
    "        x2 = self.modelB(x)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим путь для загрузки моделей!\n",
    "path_vgg16_bn = '/путь_до_весов_модели/vgg16_bn.pth'\n",
    "path_resnet50 = '/путь_до_весов_модели/resnet50.pth'\n",
    "\n",
    "# Загружаем state dicts\n",
    "model_vgg16_bn.load_state_dict(torch.load(path_vgg16_bn))\n",
    "model_resnet50.load_state_dict(torch.load(path_resnet50))\n",
    "\n",
    "# замораживаем параметры (веса) не входящие в layers_to_unfreeze\n",
    "for param in model_ensemble.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model_ensemble.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling\n",
    "\n",
    "labels = [path.parent.name for path in files] \n",
    "\n",
    "def create_dct_path_labels(train_files, train_labels):\n",
    "    dct_simpsons = {}\n",
    "    for label_i in np.unique(train_labels).tolist():\n",
    "        dct_simpsons[label_i] = []\n",
    "\n",
    "    for path_i, label_i in zip(train_files, train_labels):\n",
    "        dct_simpsons[label_i].append(path_i)\n",
    "\n",
    "    return dct_simpsons\n",
    "\n",
    "def print_dct(dct_simpsons):\n",
    "    for key in dct_simpsons:\n",
    "        print(f\"{key}\\t{dct_simpsons[key]}\")\n",
    "        \n",
    "# Создадим словарь в котором ключами будут персонажи Симпсонов, а значениями списки с путями к картинкам.\n",
    "dct_path_train = create_dct_path_labels(train_files, train_labels)\n",
    "\n",
    "# Дополним картинки классов у которых менее 75 картинок, до 75 картинок в классе\n",
    "for person in dct_path_train:\n",
    "    if len(dct_path_train[person]) < 75:\n",
    "        dct_path_train[person] = dct_path_train[person] * (75 // len(dct_path_train[person]))\n",
    "        dct_path_train[person].extend(dct_path_train[person][:75 - len(dct_path_train[person])])\n",
    "        \n",
    "new_files = []\n",
    "\n",
    "for person in dct_path_train:\n",
    "    new_files.extend(dct_path_train[person])\n",
    "\n",
    "new_labels = [path.parent.name for path in new_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очень простая сеть\n",
    "class SimpleCnn(nn.Module):\n",
    "  \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=96 * 5 * 5, out_features=600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=600, out_features=n_classes)\n",
    "        )\n",
    "  \n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.out(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем классную визуализацию, чтобы посмотреть насколько сеть уверена в своих ответах. \n",
    "# Можете исспользовать это, чтобы отлаживать правильность вывода.\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(12, 12), \\\n",
    "                        sharey=True, sharex=True)\n",
    "for fig_x in ax.flatten():\n",
    "    random_characters = int(np.random.uniform(0,1000))\n",
    "    im_val, label = val_dataset[random_characters]\n",
    "    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n",
    "                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n",
    "    \n",
    "    \n",
    "\n",
    "    imshow(im_val.data.cpu(), \\\n",
    "          title=img_label,plt_ax=fig_x)\n",
    "    \n",
    "    actual_text = \"Actual : {}\".format(img_label)\n",
    "            \n",
    "    fig_x.add_patch(patches.Rectangle((0, 53),86,35,color='white'))\n",
    "    font0 = FontProperties()\n",
    "    font = font0.copy()\n",
    "    font.set_family(\"fantasy\")\n",
    "    prob_pred = predict_one_sample(eff_net, im_val.unsqueeze(0))\n",
    "    predicted_proba = np.max(prob_pred)*100\n",
    "    y_pred = np.argmax(prob_pred)\n",
    "    \n",
    "    predicted_label = label_encoder.classes_[y_pred]\n",
    "    predicted_label = predicted_label[:len(predicted_label)//2] + '\\n' + predicted_label[len(predicted_label)//2:]\n",
    "    predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n",
    "            \n",
    "    fig_x.text(1, 59, predicted_text , horizontalalignment='left', fontproperties=font,\n",
    "                    verticalalignment='top',fontsize=8, color='black',fontweight='bold')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
